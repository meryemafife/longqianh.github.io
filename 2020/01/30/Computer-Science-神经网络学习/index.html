<!DOCTYPE html>


<html lang="en">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    神经网络学习 |  Peter&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/ironman.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  
  

  

</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content on">
      <section class="outer">
  <article id="post-Computer-Science-神经网络学习" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  神经网络学习
</h1>
 

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/01/30/Computer-Science-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-01-30T09:25:27.000Z" itemprop="datePublished">2020-01-30</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Computer-Science/">Computer Science</a> / <a class="article-category-link" href="/categories/Computer-Science/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="/categories/Computer-Science/Machine-Learning/Neural-Networks/">Neural Networks</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">2.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">14 min</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="神经网络-学习笔记">神经网络 学习笔记</h1>
<p>学习资源: <a href="">neuralnetworksanddeeplearning.com</a></p>
<h2 id="chapter-12">Chapter 1、2</h2>
<h3 id="基本概念">1.1 基本概念</h3>
<ul>
<li><p>activation function 激活函数/传递函数 定义了该节点在给定的输入或输入的集合下的输出，比如 sigmoid function</p></li>
<li><p>perceptron model 感知器：输入 <span class="math inline">\(0,1\)</span> 二值函数，输出 <span class="math inline">\(0,1\)</span> 二值函数</p></li>
<li><p>sigmoid neurons S 神经元</p>
<p>​ 中间部分更精细，<span class="math inline">\(\displaystyle\sigma(z)=\frac{1}{1+e^{-z}}\)</span> 输入二值，输出为 <span class="math inline">\([0,1]\)</span> 之间的数，极限性质和感知器一样</p></li>
<li><p>feedforward neural networks 前馈神经网络</p>
<p>​ 没有反馈的神经网络，由输入层(input layer)、隐藏层(hidden layer)、输出层(output layer)构成</p></li>
</ul>
<p>$ \ $ <a id="more"></a></p>
<h3 id="stochastic-gradient-descent-随机梯度下降">1.2 stochastic gradient descent 随机梯度下降</h3>
<h4 id="参数">1.2.1 参数</h4>
<ul>
<li><span class="math inline">\(x\)</span> ：输入的训练集（一个矩阵，每一列是一次单独的输入）, <span class="math inline">\(y\)</span> ：最终输出，<span class="math inline">\(y=w\cdot x+b\)</span></li>
<li><span class="math inline">\(w\)</span>：weights，由矩阵组成的向量。<span class="math inline">\(w_j\)</span> 是一个 <span class="math inline">\(m\times n\)</span> 型矩阵，连接 <span class="math inline">\(j-1\)</span> 层和 <span class="math inline">\(j\)</span> 层，其中 <span class="math inline">\(j-1\)</span> 层有 <span class="math inline">\(n\)</span> 个神经元，$ j $ 层有 <span class="math inline">\(m\)</span> 个神经元</li>
<li><span class="math inline">\(b\)</span> ：biases，由 $ m1$ 型矩阵组成的向量。 <span class="math inline">\(b_k\)</span> 是一个<span class="math inline">\(m\times 1\)</span> 型矩阵，连接 <span class="math inline">\(j-1\)</span> 层和 <span class="math inline">\(j\)</span> 层，其中第 <span class="math inline">\(j\)</span> 层有 <span class="math inline">\(m\)</span> 个神经元</li>
<li>Cost function ： $ C(w,b)=<em>x |y(x)-a |^2=</em>{i=1}<sup>{n}[y(x<sup>{(i)})-a</sup>{(i)}]</sup>2 $</li>
</ul>
<h4 id="方法">1.2.2 方法</h4>
<ul>
<li>对于整个训练集，用随机梯度下降法使损失函数尽可能小</li>
<li>先将训练集打乱，再按给定的小批训练集大小等间隔取小批训练集（相当于随机取样），每次选择更优的 <span class="math inline">\(w,b\)</span> 使损失函数一步步减小</li>
</ul>
<h4 id="保证每次都获得更优的-wb">1.2.3 保证每次都获得更优的 <span class="math inline">\(w,b\)</span>：</h4>
<ol type="1">
<li></li>
</ol>
<p><span class="math display">\[
\displaystyle \nabla C=(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T , \quad \vec v_1\equiv v_1\equiv w,\vec v_2\equiv v_2\equiv b\\ \displaystyle\Delta C\approx  \frac{\partial C}{\partial w}\Delta v_1+\frac{\partial C}{\partial b}\Delta v_2=\Delta v\cdot \nabla C \\ 
=&gt; take\quad \Delta v=(\Delta v_1,\Delta v_2)=-\eta \nabla C,\quad \eta&gt;0\\
\begin{aligned}
\therefore C&amp;:=C+\Delta C=C-\eta \|\nabla C \|^2\\
v_1&amp; :=v_1-\eta\cdot\frac{\partial C}{\partial v_1}\\
v_2&amp;:=v_2-\eta\cdot\frac{\partial C}{\partial v_2s}
\end{aligned}
\]</span></p>
<ol start="2" type="1">
<li>分量式：</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
w_{jk}&amp;:=w_{jk}-\eta\cdot \frac{\partial C}{\partial w_{jk}}=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(i)}} \\
b_{k}&amp;:=b_{ k}-\eta\cdot \frac{\partial C}{\partial b_{k}}=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(i)}}
\end{aligned}
\]</span></p>
<h2 id="bp算法反向传播-back-propagate">2 BP算法（反向传播 back propagate ）</h2>
<p>目的：快速计算 <span class="math inline">\(\displaystyle \frac{\partial C}{\partial w},\frac{\partial C}{\partial b}\)</span> .</p>
<h3 id="原理">2.1 原理：</h3>
<p>最终结果产生的偏差是由于每个神经元产生的误差叠加起来造成的。通过前向反馈得到的结果若与正确结果有偏差，则从此偏差反向推导，可得到各层神经元产生的误差。</p>
<h3 id="符号表示">2.2 符号表示</h3>
<ul>
<li><span class="math inline">\(w_{jk}^l\)</span> ：连接第 <span class="math inline">\(l-1\)</span>层第 <span class="math inline">\(k\)</span> 个神经元和第 <span class="math inline">\(l\)</span> 层第 <span class="math inline">\(j\)</span> 个神经元的权重</li>
<li><span class="math inline">\(w^l\)</span> ：连接第 <span class="math inline">\(l-1\)</span> 和第 <span class="math inline">\(l\)</span> 层网络的权重矩阵</li>
<li><span class="math inline">\(b_j^l\)</span> ：第 <span class="math inline">\(l\)</span> 层第 <span class="math inline">\(j\)</span> 个神经元的 bias</li>
<li><span class="math inline">\(z_j^l\)</span> ：第 <span class="math inline">\(l\)</span> 层网络的带权输入 <span class="math inline">\(z_j^l=\displaystyle\sum_k w_{jk}^l a_k^{l-1}+b_j^l\)</span>，<span class="math inline">\(z^l=w^l a^{l-1}+b^l\)</span></li>
<li><p><span class="math inline">\(\delta_j^l\)</span> ：第 <span class="math inline">\(l\)</span> 层第 <span class="math inline">\(j\)</span> 个神经元产生的误差,，定义 <span class="math inline">\(\displaystyle \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}\)</span></p></li>
<li><span class="math inline">\(a_j^l\)</span> ：第 <span class="math inline">\(l\)</span> 层网络的第 <span class="math inline">\(j\)</span> 个神经元的激活量，<span class="math inline">\(a_j^l=\sigma(z_j^l)\)</span></li>
<li><p><span class="math inline">\(C\)</span> ：损失函数，用小批量输入的平均值近似。</p></li>
</ul>
<h3 id="重要的关系公式">2.3 重要的关系公式</h3>
<ol start="0" type="1">
<li><p>注：推导中的除法 ''<span class="math inline">\(\bigg /\)</span> ''、乘法 <span class="math inline">\(\odot\)</span> （ Hadamard Product）均为<strong>elementwise</strong></p></li>
<li><p><span class="math display">\[
C=\displaystyle \frac{1}{2n}\sum_x \|y(x)-a^L(x)\|^2=\frac{1}{n}\sum_x ^n C(x)\approx\frac{1}{m}\sum_x^m C(x)
\\ C(x)=\frac{1}{2}\|y-a^L(x)\|^2=\frac{1}{2}\sum_i \big(y_i-a_i^L(x)\big)^2
\]</span></p></li>
<li><p><span class="math display">\[
a_j^l=\sigma\bigg(\sum_k w_{jk}^l a_k^{l-1}+b_j^l\bigg)
\\ a^l=\sigma\bigg(w^l a^{l-1}+b^l\bigg)
\]</span></p></li>
<li><p><span class="math display">\[
\delta^l=\frac{\partial C}{\partial b^l}\bigg/ \frac{\partial z^l}{\partial b^l}=\frac{\partial C}{\partial b^l},\qquad \frac{\partial C}{\partial b^l}\triangleq(\frac{\partial C}{\partial b_1^l},\cdots,\frac{\partial C}{\partial b_{N(l)}^l})^T \tag{1}
\]</span></p></li>
<li><p><span class="math display">\[
\delta_j^l=\frac{\partial C}{\partial w_{jk}^l}\bigg /\frac{\partial z^l}{\partial w_{jk}^l}=\frac{\partial C}{\partial w_{jk}^l}\big/ a_k^{l-1}
\]</span></p></li>
</ol>
<p><span class="math display">\[
i .e. \qquad \frac{\partial C}{\partial w_{jk}^l}=a_{k}^{l-1}\cdot  \delta_j^l \qquad =&gt;\nabla C_{w^l}=\delta^l\cdot (a^{l-1})^T \tag{2}
\]</span></p>
<ol start="5" type="1">
<li><p><span class="math display">\[
\delta^L=\frac{\partial C}{\partial a^L}\odot \frac{\partial a^L}{\partial z^L}=\frac{\partial C}{\partial a^L}\odot\sigma&#39;(z^L)=(a^L-y) \odot \sigma&#39;(z^L)\tag{3}
\]</span></p></li>
<li><p><strong>Move error backward:</strong> <span class="math display">\[
\begin{eqnarray}
z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k
=&gt;\frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma&#39;(z^l_j)
\end{eqnarray}
\]</span></p>
<p><span class="math display">\[
\delta_j^l=\sum_k \frac{\partial C}{\partial z_k^{l+1}}\cdot \frac{\partial z_k^{l+1}}{\partial z_j^l}=\sum_{k=1}^{N(l)}\delta_k^{l+1}\cdot w_{kj}^{l+1}\cdot \sigma&#39;(z_j^l)
\]</span></p>
<p><span class="math display">\[
=&gt; \delta^l=\big((w^{l+1})^T\sigma&#39;(z^l)\big)\odot \delta^{l+1}\tag{4}
\]</span></p></li>
</ol>
<h3 id="实现步骤">2.4 实现步骤</h3>
<ol type="1">
<li><p>feedforward : 得到 <span class="math inline">\(z^L\)</span> 和相应的 <span class="math inline">\(a^L\)</span> <span class="math display">\[
a^{x,l} = \sigma(z^{x,l})
\]</span></p></li>
<li><p>output error : <span class="math display">\[
\delta^{x,L} = \nabla_a C_x \odot \sigma&#39;(z^{x,L})
\]</span></p></li>
<li><p>error back propagate :<br />
<span class="math display">\[
\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})
  \odot \sigma&#39;(z^{x,l})
\]</span></p></li>
<li><p>update weights and biases : <span class="math display">\[
\begin{aligned}
w_{jk}&amp;:=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(x_i)}}=w_{jk}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(a_k^{l-1}\delta_j^{l}\big)^{(x_i)}\\
b_{k}&amp;:=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(x_i)}}=b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(\delta_k^{l}\big)^{(x_i)}
\end{aligned}
\]</span></p>
<p>即： <span class="math display">\[
w^l \rightarrow
  w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T
 \\b^l \rightarrow b^l-\frac{\eta}{m}
  \sum_x \delta^{x,l}
\]</span></p></li>
</ol>
<h3 id="代码实现">2.5 代码实现</h3>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code">
<span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a>
</p>
<pre class="sourceCode python" id="cb1"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> random</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">class</span> Network(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sizes):</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">        <span class="co"># 生成神经网络 </span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">        <span class="va">self</span>.num_layers <span class="op">=</span> <span class="bu">len</span>(sizes)</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">        <span class="va">self</span>.sizes <span class="op">=</span> sizes</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">        <span class="va">self</span>.biases <span class="op">=</span> [np.random.randn(y, <span class="dv">1</span>) <span class="cf">for</span> y <span class="kw">in</span> sizes[<span class="dv">1</span>:]]</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">        <span class="va">self</span>.weights <span class="op">=</span> [np.random.randn(y, x)</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">                        <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(sizes[:<span class="op">-</span><span class="dv">1</span>], sizes[<span class="dv">1</span>:])]</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"></a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    <span class="kw">def</span> feedforward(<span class="va">self</span>, a):</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">        <span class="co"># 激活神经元，向后面的层产生反馈</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17">        <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</a>
<a class="sourceLine" id="cb1-18" data-line-number="18">            a <span class="op">=</span> sigmoid(np.dot(w, a) <span class="op">+</span> b)</a>
<a class="sourceLine" id="cb1-19" data-line-number="19">        <span class="cf">return</span> a</a>
<a class="sourceLine" id="cb1-20" data-line-number="20"></a>
<a class="sourceLine" id="cb1-21" data-line-number="21">    <span class="kw">def</span> SGD(<span class="va">self</span>, training_data, epochs, mini_batch_size, eta,</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">            test_data<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">        <span class="co"># 随机梯度下降 Stochastic Gradient Descent</span></a>
<a class="sourceLine" id="cb1-24" data-line-number="24">        <span class="co"># epochs 训练次数, mini_batch_size 每次选取的小训练集大小, eta 学习率 由自己设定</span></a>
<a class="sourceLine" id="cb1-25" data-line-number="25">        <span class="cf">if</span> test_data:</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">            test_data <span class="op">=</span> <span class="bu">list</span>(test_data)  </a>
<a class="sourceLine" id="cb1-27" data-line-number="27">            n_test <span class="op">=</span> <span class="bu">len</span>(test_data)</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">        training_data <span class="op">=</span> <span class="bu">list</span>(training_data)</a>
<a class="sourceLine" id="cb1-29" data-line-number="29">        n <span class="op">=</span> <span class="bu">len</span>(training_data)</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">        </a>
<a class="sourceLine" id="cb1-31" data-line-number="31">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</a>
<a class="sourceLine" id="cb1-32" data-line-number="32">            random.shuffle(training_data)</a>
<a class="sourceLine" id="cb1-33" data-line-number="33">            mini_batches <span class="op">=</span> [</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">                training_data[k:k <span class="op">+</span> mini_batch_size]</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">                <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, mini_batch_size)]  </a>
<a class="sourceLine" id="cb1-36" data-line-number="36">            <span class="cf">for</span> mini_batch <span class="kw">in</span> mini_batches:</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">                <span class="va">self</span>.update_mini_batch(mini_batch, eta)</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">            <span class="cf">if</span> test_data:</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">                <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{0}</span><span class="st">: </span><span class="sc">{1}</span><span class="st"> / </span><span class="sc">{2}</span><span class="st">&quot;</span>.<span class="bu">format</span>(j,</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">                                                    <span class="va">self</span>.evaluate(test_data), n_test))</a>
<a class="sourceLine" id="cb1-41" data-line-number="41"></a>
<a class="sourceLine" id="cb1-42" data-line-number="42">            <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">                <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{0}</span><span class="st"> complete&quot;</span>.<span class="bu">format</span>(j))</a>
<a class="sourceLine" id="cb1-44" data-line-number="44"></a>
<a class="sourceLine" id="cb1-45" data-line-number="45">    <span class="kw">def</span> update_mini_batch(<span class="va">self</span>, mini_batch, eta):</a>
<a class="sourceLine" id="cb1-46" data-line-number="46">        <span class="co"># 更新 w 和 b</span></a>
<a class="sourceLine" id="cb1-47" data-line-number="47">        nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">        nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">        <span class="cf">for</span> x, y <span class="kw">in</span> mini_batch:</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">            delta_nabla_b, delta_nabla_w <span class="op">=</span> <span class="va">self</span>.backprop(x, y)</a>
<a class="sourceLine" id="cb1-51" data-line-number="51">            nabla_b <span class="op">=</span> [nb <span class="op">+</span> dnb <span class="cf">for</span> nb, dnb <span class="kw">in</span> <span class="bu">zip</span>(nabla_b, delta_nabla_b)]</a>
<a class="sourceLine" id="cb1-52" data-line-number="52">            nabla_w <span class="op">=</span> [nw <span class="op">+</span> dnw <span class="cf">for</span> nw, dnw <span class="kw">in</span> <span class="bu">zip</span>(nabla_w, delta_nabla_w)]</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">        <span class="va">self</span>.weights <span class="op">=</span> [w <span class="op">-</span> (eta <span class="op">/</span> <span class="bu">len</span>(mini_batch)) <span class="op">*</span> nw</a>
<a class="sourceLine" id="cb1-54" data-line-number="54">                        <span class="cf">for</span> w, nw <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.weights, nabla_w)]</a>
<a class="sourceLine" id="cb1-55" data-line-number="55">        <span class="va">self</span>.biases <span class="op">=</span> [b <span class="op">-</span> (eta <span class="op">/</span> <span class="bu">len</span>(mini_batch)) <span class="op">*</span> nb</a>
<a class="sourceLine" id="cb1-56" data-line-number="56">                       <span class="cf">for</span> b, nb <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, nabla_b)]</a>
<a class="sourceLine" id="cb1-57" data-line-number="57"></a>
<a class="sourceLine" id="cb1-58" data-line-number="58">    <span class="kw">def</span> backprop(<span class="va">self</span>, x, y):</a>
<a class="sourceLine" id="cb1-59" data-line-number="59">        <span class="co"># 反向传播算法</span></a>
<a class="sourceLine" id="cb1-60" data-line-number="60">        nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</a>
<a class="sourceLine" id="cb1-61" data-line-number="61">        nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</a>
<a class="sourceLine" id="cb1-62" data-line-number="62">        <span class="co"># feedforward</span></a>
<a class="sourceLine" id="cb1-63" data-line-number="63">        activation <span class="op">=</span> x</a>
<a class="sourceLine" id="cb1-64" data-line-number="64">        activations <span class="op">=</span> [x]  <span class="co"># list to store all the activations, layer by layer</span></a>
<a class="sourceLine" id="cb1-65" data-line-number="65">        zs <span class="op">=</span> []  <span class="co"># list to store all the z vectors, layer by layer</span></a>
<a class="sourceLine" id="cb1-66" data-line-number="66">        <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</a>
<a class="sourceLine" id="cb1-67" data-line-number="67">            z <span class="op">=</span> np.dot(w, activation) <span class="op">+</span> b</a>
<a class="sourceLine" id="cb1-68" data-line-number="68">            zs.append(z)</a>
<a class="sourceLine" id="cb1-69" data-line-number="69">            activation <span class="op">=</span> sigmoid(z)</a>
<a class="sourceLine" id="cb1-70" data-line-number="70">            activations.append(activation)</a>
<a class="sourceLine" id="cb1-71" data-line-number="71">        <span class="co"># backward pass</span></a>
<a class="sourceLine" id="cb1-72" data-line-number="72">        delta <span class="op">=</span> <span class="va">self</span>.cost_derivative(activations[<span class="op">-</span><span class="dv">1</span>], y) <span class="op">*</span> <span class="op">\</span></a>
<a class="sourceLine" id="cb1-73" data-line-number="73">            sigmoid_prime(zs[<span class="op">-</span><span class="dv">1</span>])<span class="co"># \delta^L</span></a>
<a class="sourceLine" id="cb1-74" data-line-number="74">        nabla_b[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> delta</a>
<a class="sourceLine" id="cb1-75" data-line-number="75">        nabla_w[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> np.dot(delta, activations[<span class="op">-</span><span class="dv">2</span>].transpose())</a>
<a class="sourceLine" id="cb1-76" data-line-number="76"></a>
<a class="sourceLine" id="cb1-77" data-line-number="77">        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="va">self</span>.num_layers):</a>
<a class="sourceLine" id="cb1-78" data-line-number="78">            z <span class="op">=</span> zs[<span class="op">-</span>l]</a>
<a class="sourceLine" id="cb1-79" data-line-number="79">            sp <span class="op">=</span> sigmoid_prime(z)</a>
<a class="sourceLine" id="cb1-80" data-line-number="80">            delta <span class="op">=</span> np.dot(<span class="va">self</span>.weights[<span class="op">-</span>l <span class="op">+</span> <span class="dv">1</span>].transpose(), delta) <span class="op">*</span> sp</a>
<a class="sourceLine" id="cb1-81" data-line-number="81">            nabla_b[<span class="op">-</span>l] <span class="op">=</span> delta</a>
<a class="sourceLine" id="cb1-82" data-line-number="82">            nabla_w[<span class="op">-</span>l] <span class="op">=</span> np.dot(delta, activations[<span class="op">-</span>l <span class="op">-</span> <span class="dv">1</span>].transpose())</a>
<a class="sourceLine" id="cb1-83" data-line-number="83">        <span class="cf">return</span> (nabla_b, nabla_w)</a>
<a class="sourceLine" id="cb1-84" data-line-number="84"></a>
<a class="sourceLine" id="cb1-85" data-line-number="85">    <span class="kw">def</span> evaluate(<span class="va">self</span>, test_data):</a>
<a class="sourceLine" id="cb1-86" data-line-number="86">       <span class="co"># 返回输出结果和训练数据集的正确结果相同的个数</span></a>
<a class="sourceLine" id="cb1-87" data-line-number="87">        test_results <span class="op">=</span> [(np.argmax(<span class="va">self</span>.feedforward(x)), y)</a>
<a class="sourceLine" id="cb1-88" data-line-number="88">                        <span class="cf">for</span> (x, y) <span class="kw">in</span> test_data]</a>
<a class="sourceLine" id="cb1-89" data-line-number="89">        <span class="cf">return</span> <span class="bu">sum</span>(<span class="bu">int</span>(x <span class="op">==</span> y) <span class="cf">for</span> (x, y) <span class="kw">in</span> test_results) </a>
<a class="sourceLine" id="cb1-90" data-line-number="90"></a>
<a class="sourceLine" id="cb1-91" data-line-number="91">    <span class="kw">def</span> cost_derivative(<span class="va">self</span>, output_activations, y):</a>
<a class="sourceLine" id="cb1-92" data-line-number="92">        <span class="co">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></a>
<a class="sourceLine" id="cb1-93" data-line-number="93"><span class="co">        \partial a for the output activations.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-94" data-line-number="94">        <span class="cf">return</span> (output_activations <span class="op">-</span> y)</a>
<a class="sourceLine" id="cb1-95" data-line-number="95"></a>
<a class="sourceLine" id="cb1-96" data-line-number="96"><span class="co"># Miscellaneous functions</span></a>
<a class="sourceLine" id="cb1-97" data-line-number="97"></a>
<a class="sourceLine" id="cb1-98" data-line-number="98"><span class="kw">def</span> sigmoid(z):</a>
<a class="sourceLine" id="cb1-99" data-line-number="99">    <span class="co"># The sigmoid function.</span></a>
<a class="sourceLine" id="cb1-100" data-line-number="100">    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</a>
<a class="sourceLine" id="cb1-101" data-line-number="101"></a>
<a class="sourceLine" id="cb1-102" data-line-number="102"></a>
<a class="sourceLine" id="cb1-103" data-line-number="103"><span class="kw">def</span> sigmoid_prime(z):</a>
<a class="sourceLine" id="cb1-104" data-line-number="104">    <span class="co"># Derivative of the sigmoid function.</span></a>
<a class="sourceLine" id="cb1-105" data-line-number="105">    <span class="cf">return</span> sigmoid(z) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid(z))</a></code></pre>
<h2 id="chapter-3">Chapter 3</h2>
<h3 id="交叉熵">3.1 交叉熵</h3>
<p>对错误的乘法力度加大，学习速率更快</p>
<h3 id="softmax">3.2 Softmax</h3>
<p>输出转化为概略分布</p>
<h3 id="其他损失函数">3.3 其他损失函数</h3>
<h3 id="调整超参数的方法">3.4 调整超参数的方法</h3>
<h2 id="chapter-4">Chapter 4</h2>
<p>神经网络可以计算任何函数。</p>
<h2 id="chapter-5">Chapter 5</h2>
<ul>
<li><p>vanishing gradient</p></li>
<li><p>exploding gradient</p></li>
</ul>
<h2 id="chapter-6-cnn">Chapter 6 CNN</h2>
<h3 id="架构">6.1 架构</h3>
<ul>
<li>卷积层
<ul>
<li>共享权重</li>
<li>提取特征</li>
</ul></li>
<li>池化层
<ul>
<li>减少参量</li>
</ul></li>
<li>全连层
<ul>
<li>二维压缩成一维，方便输出</li>
</ul></li>
<li>Softmax层
<ul>
<li>输出概率分布</li>
</ul></li>
</ul>
<h3 id="代码-with-pytorch">6.2 代码： with pytorch</h3>
<h4 id="数据载入">6.2.1 数据载入</h4>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code">
<span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a>
</p>
<pre class="sourceCode python" id="cb2"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">train_data <span class="op">=</span> torchvision.datasets.MNIST(</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    root<span class="op">=</span><span class="st">&#39;./&#39;</span>,    <span class="co"># 保存或者提取位置</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    train<span class="op">=</span><span class="va">True</span>, </a>
<a class="sourceLine" id="cb2-4" data-line-number="4">    <span class="co"># 转换 PIL.Image or numpy.ndarray</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">    transform<span class="op">=</span>torchvision.transforms.ToTensor(),</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">    download<span class="op">=</span>DOWNLOAD_MNIST,</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">)</a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="co"># print(train_data.data.shape)</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">#train_data.data : (60000,28,28)</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="co"># train_data.targets : (60000)</span></a>
<a class="sourceLine" id="cb2-11" data-line-number="11">test_data <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;./&#39;</span>, train<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="co"># test_data 包括：test_data.data 和 test_data.targets</span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="co">#test_data.data : (10000,28,28)</span></a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="co">#test_data.targets : (10000)</span></a>
<a class="sourceLine" id="cb2-15" data-line-number="15"></a>
<a class="sourceLine" id="cb2-16" data-line-number="16"><span class="co"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></a>
<a class="sourceLine" id="cb2-17" data-line-number="17">train_batches <span class="op">=</span> Data.DataLoader(</a>
<a class="sourceLine" id="cb2-18" data-line-number="18">    dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="co"># print(train_batches.dataset.targets.shape</span></a>
<a class="sourceLine" id="cb2-20" data-line-number="20"></a>
<a class="sourceLine" id="cb2-21" data-line-number="21"><span class="co"># 测试前2000个</span></a>
<a class="sourceLine" id="cb2-22" data-line-number="22">test_x <span class="op">=</span> torch.unsqueeze(test_data.data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(torch.FloatTensor)[</a>
<a class="sourceLine" id="cb2-23" data-line-number="23">    :<span class="dv">2000</span>] <span class="op">/</span> <span class="fl">255.0</span>  <span class="co"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></a>
<a class="sourceLine" id="cb2-24" data-line-number="24">test_y <span class="op">=</span> test_data.targets[:<span class="dv">2000</span>]</a></code></pre>
<h4 id="卷积神经网络构建">6.2.2 卷积神经网络构建</h4>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code">
<span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a>
</p>
<pre class="sourceCode python" id="cb3"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">class</span> CNN(nn.Module):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">        <span class="bu">super</span>(CNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Sequential(  <span class="co"># input shape (1, 28, 28)</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">            nn.Conv2d(</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">                in_channels<span class="op">=</span><span class="dv">1</span>,      <span class="co"># input height</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">                out_channels<span class="op">=</span><span class="dv">16</span>,    <span class="co"># n_filters i.e. number of features</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8">                kernel_size<span class="op">=</span><span class="dv">5</span>,      <span class="co"># filter size</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9">                stride<span class="op">=</span><span class="dv">1</span>,           <span class="co"># filter movement/step</span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">                <span class="co"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11">                padding<span class="op">=</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">            ),      <span class="co"># -&gt; (16, 28, 28)</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13">            nn.ReLU(),    <span class="co"># activation</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14">            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>),    <span class="co"># -&gt; (16, 14, 14)</span></a>
<a class="sourceLine" id="cb3-15" data-line-number="15">        )</a>
<a class="sourceLine" id="cb3-16" data-line-number="16">        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Sequential(  <span class="co"># input shape (16, 14, 14)</span></a>
<a class="sourceLine" id="cb3-17" data-line-number="17">            nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">2</span>),  <span class="co"># output shape (32, 14, 14)</span></a>
<a class="sourceLine" id="cb3-18" data-line-number="18">            nn.ReLU(),  <span class="co"># activation</span></a>
<a class="sourceLine" id="cb3-19" data-line-number="19">            nn.MaxPool2d(<span class="dv">2</span>),  <span class="co"># output shape (32, 7, 7)</span></a>
<a class="sourceLine" id="cb3-20" data-line-number="20">        )</a>
<a class="sourceLine" id="cb3-21" data-line-number="21">        <span class="co"># fully connected layer, output 10 classes</span></a>
<a class="sourceLine" id="cb3-22" data-line-number="22">        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb3-23" data-line-number="23">        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">100</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb3-24" data-line-number="24"></a>
<a class="sourceLine" id="cb3-25" data-line-number="25">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb3-26" data-line-number="26">        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</a>
<a class="sourceLine" id="cb3-27" data-line-number="27">        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</a>
<a class="sourceLine" id="cb3-28" data-line-number="28">        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="dv">-1</span>)   <span class="co"># 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</span></a>
<a class="sourceLine" id="cb3-29" data-line-number="29">        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc1(x))</a>
<a class="sourceLine" id="cb3-30" data-line-number="30">        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</a>
<a class="sourceLine" id="cb3-31" data-line-number="31">        <span class="cf">return</span> x</a></code></pre>
<h4 id="反向传播训练神经网络">6.2.3 反向传播、训练神经网络</h4>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code">
<span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a>
</p>
<pre class="sourceCode python" id="cb4"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1">cnn <span class="op">=</span> CNN()</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">optimizer <span class="op">=</span> torch.optim.Adam(cnn.parameters(), lr<span class="op">=</span>LR) <span class="co">#adam算法</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">loss_func <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># Softmax–Log–NLLLoss，自动输出log-softmax</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="kw">def</span> train():</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">        <span class="co"># 分配 batch data, normalize x when iterate train_loader</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">        <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_batches):  <span class="co"># enumerate 添加索引可便历</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9">            <span class="co"># print(b_x.shape)--&gt;(50,1,28,28)</span></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">            <span class="co"># print(b_y.shape)--&gt;(50)</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">            <span class="co"># 1200组</span></a>
<a class="sourceLine" id="cb4-12" data-line-number="12">            output <span class="op">=</span> cnn(b_x)               <span class="co"># cnn output</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">            loss <span class="op">=</span> loss_func(output, b_y)   <span class="co"># cross entropy loss</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">            optimizer.zero_grad()           <span class="co"># clear gradients for this training step</span></a>
<a class="sourceLine" id="cb4-15" data-line-number="15">            loss.backward()                 <span class="co"># backpropagation, compute gradients</span></a>
<a class="sourceLine" id="cb4-16" data-line-number="16">            optimizer.step()                <span class="co"># apply gradients</span></a>
<a class="sourceLine" id="cb4-17" data-line-number="17">            <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb4-18" data-line-number="18">                test_output <span class="op">=</span> cnn(test_x)</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">                pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">                    <span class="dv">1</span>].data.numpy()  <span class="co"># [0]:一行中的最大值,[1]:最大值的索引</span></a>
<a class="sourceLine" id="cb4-21" data-line-number="21">                accuracy <span class="op">=</span> <span class="bu">float</span>((pred_y <span class="op">==</span> test_y.data.numpy()).astype(</a>
<a class="sourceLine" id="cb4-22" data-line-number="22">                    <span class="bu">int</span>).<span class="bu">sum</span>()) <span class="op">/</span> <span class="bu">float</span>(test_y.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb4-23" data-line-number="23">                <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| train loss: </span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span></a>
<a class="sourceLine" id="cb4-24" data-line-number="24">                      loss.data.numpy(), <span class="st">&#39;| test accuracy: </span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb4-25" data-line-number="25">    torch.save(cnn.state_dict(), <span class="st">&quot;cnn_net_params.pkl&quot;</span>)</a>
<a class="sourceLine" id="cb4-26" data-line-number="26"></a>
<a class="sourceLine" id="cb4-27" data-line-number="27">train()</a></code></pre>
<h4 id="参数存储与加载">6.2.4 参数存储与加载</h4>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code">
<span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a>
</p>
<pre class="sourceCode python" id="cb5"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">torch.save(cnn.state_dict(), <span class="st">&quot;cnn_net_params.pkl&quot;</span>)<span class="co">#保存训练好的参数</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">cnn <span class="op">=</span> CNN()</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">cnn.load_state_dict(torch.load(<span class="st">&#39;cnn_net_params.pkl&#39;</span>))<span class="co">#加载参数</span></a></code></pre>
<p>其他：关于深度学习和图像处理卷积的定义问题：深度学习中conv2d卷积层其实是无所谓是否翻转的，因为所有的weights也就是kernel其实是随机初始化的。那么每次的更新迭代都是为了去寻找一个最合适的kernel，所以是否翻转也变的无关紧要了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
        <div class="declare">
          <ul class="post-copyright">
            <li>
              <i class="ri-copyright-line"></i>
              <strong>Copyright： </strong>
              Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
            </li>
          </ul>
        </div>
        
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://longqianh.com/2020/01/30/Computer-Science-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/notes/" rel="tag">notes</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/02/01/Mathematics-Mathematic-Models/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Mathematics Models
          
        </div>
      </a>
    
    
      <a href="/2020/01/28/Computer-Science-Python-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python Notes</div>
      </a>
    
  </nav>


  

  

  
  
  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020
        <i class="ri-heart-fill heart_icon"></i> Longqian Huang
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>

</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/cat.png" alt="Peter&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/Projects">Projects</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/Notes">Notes</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://c01dkit.com" target="_blank" rel="noopener">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Subtitle -->

<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['Mathematics, Computer Science, Physics . . .', '', ''],
      startDelay: 0,
      typeSpeed: 250,
      loop: true,
      backSpeed: false,
      showCursor: true
    });
  } catch (err) {
    console.log(err)
  }
</script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>



    
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            displayMath: [ ['$','$'], ["\\[","\\]"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js"></script>


<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['

</html>,'

</html>], ["\\(","\\)"] ],
      displayMath: [ ['$','$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>